# Bagging Algorithm on the Sonar dataset
from random import seed
from random import randrange
from csv import reader

# Load a CSV file
def load_csv(filename):
	dataset = list()
	with open(filename, 'r') as file:
		csv_reader = reader(file)
		for row in csv_reader:
			if not row:
				continue
			dataset.append(row)
	return dataset

# Convert string column to float
def str_column_to_float(dataset, column):
	for row in dataset:
		row[column] = float(row[column].strip())

# Convert string column to integer
def str_column_to_int(dataset, column):
	class_values = [row[column] for row in dataset]
	unique = set(class_values)
	lookup = dict()
	for i, value in enumerate(unique):
		lookup[value] = i
	for row in dataset:
		row[column] = lookup[row[column]]
	return lookup

# Split a dataset into k folds
def cross_validation_split(dataset, n_folds):
	dataset_split = list()
	dataset_copy = list(dataset)
	fold_size = int(len(dataset) / n_folds)
	for i in range(n_folds):
		fold = list()
		while len(fold) < fold_size:
			index = randrange(len(dataset_copy))
			fold.append(dataset_copy.pop(index))
		dataset_split.append(fold)
	return dataset_split

# Calculate accuracy percentage
def accuracy_metric(actual, predicted):
	correct = 0
	for i in range(len(actual)):
		if actual[i] == predicted[i]:
			correct += 1
	return correct / float(len(actual)) * 100.0

# Evaluate an algorithm using a cross validation split
def evaluate_algorithm(dataset, algorithm, n_folds, *args):
	folds = cross_validation_split(dataset, n_folds)
	scores = list()
	for fold in folds:
		train_set = list(folds)
		train_set.remove(fold)
		train_set = sum(train_set, [])
		test_set = list()
		for row in fold:
			row_copy = list(row)
			test_set.append(row_copy)
			row_copy[-1] = None
		#print(train_set)
		#print(test_set)
		predicted = algorithm(train_set, test_set, *args)
		actual = [row[-1] for row in fold]
		accuracy = accuracy_metric(actual, predicted)
		scores.append(accuracy)
	return scores

# Split a dataset based on an attribute and an attribute value
def test_split(index, value, dataset):
	left, right = list(), list()
	for row in dataset:
		if row[index] < value:
			left.append(row)
		else:
			right.append(row)
	return left, right

# Calculate the Gini index for a split dataset
def gini_index(groups, class_values):
	gini = 0.0
	for class_value in class_values:
		for group in groups:
			size = len(group)
			if size == 0:
				continue
			proportion = [row[-1] for row in group].count(class_value) / float(size)
			gini += (proportion * (1.0 - proportion))
	return gini

# Select the best split point for a dataset
def get_split(dataset):
	class_values = list(set(row[-1] for row in dataset))
	b_index, b_value, b_score, b_groups = 999, 999, 999, None
	for index in range(len(dataset[0])-1):
		for row in dataset:
		# for i in range(len(dataset)):
		# 	row = dataset[randrange(len(dataset))]
			groups = test_split(index, row[index], dataset)
			gini = gini_index(groups, class_values)
			if gini < b_score:
				b_index, b_value, b_score, b_groups = index, row[index], gini, groups
	return {'index':b_index, 'value':b_value, 'groups':b_groups}

# Create a terminal node value
def to_terminal(group):
	outcomes = [row[-1] for row in group]
	return max(set(outcomes), key=outcomes.count)

# Create child splits for a node or make terminal
def split(node, max_depth, min_size, depth):
	left, right = node['groups']
	del(node['groups'])
	# check for a no split
	if not left or not right:
		node['left'] = node['right'] = to_terminal(left + right)
		return
	# check for max depth
	if depth >= max_depth:
		node['left'], node['right'] = to_terminal(left), to_terminal(right)
		return
	# process left child
	if len(left) <= min_size:
		node['left'] = to_terminal(left)
	else:
		node['left'] = get_split(left)
		split(node['left'], max_depth, min_size, depth+1)
	# process right child
	if len(right) <= min_size:
		node['right'] = to_terminal(right)
	else:
		node['right'] = get_split(right)
		split(node['right'], max_depth, min_size, depth+1)

# Build a decision tree
def build_tree(train, max_depth, min_size):
	root = get_split(train)
	split(root, max_depth, min_size, 1)
	return root

# Make a prediction with a decision tree
def predict(node, row):
	if row[node['index']] < node['value']:
		if isinstance(node['left'], dict):
			return predict(node['left'], row)
		else:
			return node['left']
	else:
		if isinstance(node['right'], dict):
			return predict(node['right'], row)
		else:
			return node['right']

# Create a random subsample from the dataset with replacement
def subsample(dataset, ratio):
	sample = list()
	n_sample = round(len(dataset) * ratio)
	while len(sample) < n_sample:
		index = randrange(len(dataset))
		sample.append(dataset[index])
	return sample

# Make a prediction with a list of bagged trees
def bagging_predict(trees, row):
	predictions = [predict(tree, row) for tree in trees]
	return max(set(predictions), key=predictions.count)

# Bootstrap Aggregation Algorithm
def bagging(train, test, max_depth, min_size, sample_size, n_trees):
	trees = list()
	for i in range(n_trees):
		sample = subsample(train, sample_size)
		tree = build_tree(sample, max_depth, min_size)
		trees.append(tree)
	predictions = [bagging_predict(trees, row) for row in test]
	return(predictions)

# Test bagging on the sonar dataset
seed(1)
# load and prepare data

import os
os.chdir(os.path.dirname(os.path.realpath(__file__)))


filename = '../Data/sonar.csv'
dataset = load_csv(filename)
print(type(dataset))
# convert string attributes to integers
for i in range(len(dataset[0])-1):
	str_column_to_float(dataset, i)
# convert class column to integers
str_column_to_int(dataset, len(dataset[0])-1)
# evaluate algorithm


'''
n_folds = 5
max_depth = 6
min_size = 2
sample_size = 0.50

trees = build_tree(dataset, max_depth, min_size)


for n_trees in [1, 5, 10, 50]:
	#scores = evaluate_algorithm(dataset, bagging, n_folds, max_depth, min_size, sample_size, n_trees)
	#print('Trees: %d' % n_trees)
	#print('Scores: %s' % scores)
	#print(n_trees)
	print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))
'''
n_folds = 5
max_depth = 6
min_size = 2
sample_size = 0.50
dataset = [[0.0093, 0.0185, 0.0056, 0.0064, 0.026, 0.0458, 0.047, 0.0057, 0.0425, 0.064, 0.0888, 0.1599, 0.1541, 0.2768, 0.2176, 0.2799, 0.3491, 0.2824, 0.2479, 0.3005, 0.43, 0.4684, 0.452, 0.5026, 0.6217, 0.6571, 0.6632, 0.7321, 0.8534, 1.0, 0.8448, 0.6354, 0.6308, 0.6211, 0.6976, 0.5868, 0.4889, 0.3683, 0.2043, 0.1469, 0.222, 0.1449, 0.149, 0.1211, 0.1144, 0.0791, 0.0365, 0.0152, 0.0085, 0.012, 0.0022, 0.0069, 0.0064, 0.0129, 0.0114, 0.0054, 0.0089, 0.005, 0.0058, 0.0025, 0.0], [0.0629, 0.1065, 0.1526, 0.1229, 0.1437, 0.119, 0.0884, 0.0907, 0.2107, 0.3597, 0.5466, 0.5205, 0.5127, 0.5395, 0.6558, 0.8705, 0.9786, 0.9335, 0.7917, 0.7383, 0.6908, 0.385, 0.0671, 0.0502, 0.2717, 0.2839, 0.2234, 0.1911, 0.0408, 0.2531, 0.1979, 0.1891, 0.2433, 0.1956, 0.2667, 0.134, 0.1073, 0.2023, 0.1794, 0.0227, 0.1313, 0.1775, 0.1549, 0.1626, 0.0708, 0.0129, 0.0795, 0.0762, 0.0117, 0.0061, 0.0257, 0.0089, 0.0262, 0.0108, 0.0138, 0.0187, 0.023, 0.0057, 0.0113, 0.0131, 1.0], [0.0217, 0.0152, 0.0346, 0.0346, 0.0484, 0.0526, 0.0773, 0.0862, 0.1451, 0.211, 0.2343, 0.2087, 0.1645, 0.1689, 0.165, 0.1967, 0.2934, 0.3709, 0.4309, 0.4161, 0.5116, 0.6501, 0.7717, 0.8491, 0.9104, 0.8912, 0.8189, 0.6779, 0.5368, 0.5207, 0.5651, 0.5749, 0.525, 0.4255, 0.333, 0.2331, 0.1451, 0.1648, 0.2694, 0.373, 0.4467, 0.4133, 0.3743, 0.3021, 0.2069, 0.179, 0.1689, 0.1341, 0.0769, 0.0222, 0.0205, 0.0123, 0.0067, 0.0011, 0.0026, 0.0049, 0.0029, 0.0022, 0.0022, 0.0032, 1.0], [0.0968, 0.0821, 0.0629, 0.0608, 0.0617, 0.1207, 0.0944, 0.4223, 0.5744, 0.5025, 0.3488, 0.17, 0.2076, 0.3087, 0.4224, 0.5312, 0.2436, 0.1884, 0.1908, 0.8321, 1.0, 0.4076, 0.096, 0.1928, 0.2419, 0.379, 0.2893, 0.3451, 0.3777, 0.5213, 0.2316, 0.3335, 0.4781, 0.6116, 0.6705, 0.7375, 0.7356, 0.7792, 0.6788, 0.5259, 0.2762, 0.1545, 0.2019, 0.2231, 0.4221, 0.3067, 0.1329, 0.1349, 0.1057, 0.0499, 0.0206, 0.0073, 0.0081, 0.0303, 0.019, 0.0212, 0.0126, 0.0201, 0.021, 0.0041, 1.0], [0.026, 0.0363, 0.0136, 0.0272, 0.0214, 0.0338, 0.0655, 0.14, 0.1843, 0.2354, 0.272, 0.2442, 0.1665, 0.0336, 0.1302, 0.1708, 0.2177, 0.3175, 0.3714, 0.4552, 0.57, 0.7397, 0.8062, 0.8837, 0.9432, 1.0, 0.9375, 0.7603, 0.7123, 0.8358, 0.7622, 0.4567, 0.1715, 0.1549, 0.1641, 0.1869, 0.2655, 0.1713, 0.0959, 0.0768, 0.0847, 0.2076, 0.2505, 0.1862, 0.1439, 0.147, 0.0991, 0.0041, 0.0154, 0.0116, 0.0181, 0.0146, 0.0129, 0.0047, 0.0039, 0.0061, 0.004, 0.0036, 0.0061, 0.0115, 1.0], [0.034, 0.0625, 0.0381, 0.0257, 0.0441, 0.1027, 0.1287, 0.185, 0.2647, 0.4117, 0.5245, 0.5341, 0.5554, 0.3915, 0.295, 0.3075, 0.3021, 0.2719, 0.5443, 0.7932, 0.8751, 0.8667, 0.7107, 0.6911, 0.7287, 0.8792, 1.0, 0.9816, 0.8984, 0.6048, 0.4934, 0.5371, 0.4586, 0.2908, 0.0774, 0.2249, 0.1602, 0.3958, 0.6117, 0.5196, 0.2321, 0.437, 0.3797, 0.4322, 0.4892, 0.1901, 0.094, 0.1364, 0.0906, 0.0144, 0.0329, 0.0141, 0.0019, 0.0067, 0.0099, 0.0042, 0.0057, 0.0051, 0.0033, 0.0058, 1.0], [0.0264, 0.0071, 0.0342, 0.0793, 0.1043, 0.0783, 0.1417, 0.1176, 0.0453, 0.0945, 0.1132, 0.084, 0.0717, 0.1968, 0.2633, 0.4191, 0.505, 0.6711, 0.7922, 0.8381, 0.8759, 0.9422, 1.0, 0.9931, 0.9575, 0.8647, 0.7215, 0.5801, 0.4964, 0.4886, 0.4079, 0.2443, 0.1768, 0.2472, 0.3518, 0.3762, 0.2909, 0.2311, 0.3168, 0.3554, 0.3741, 0.4443, 0.3261, 0.1963, 0.0864, 0.1688, 0.1991, 0.1217, 0.0628, 0.0323, 0.0253, 0.0214, 0.0262, 0.0177, 0.0037, 0.0068, 0.0121, 0.0077, 0.0078, 0.0066, 1.0], [0.0526, 0.0563, 0.1219, 0.1206, 0.0246, 0.1022, 0.0539, 0.0439, 0.2291, 0.1632, 0.2544, 0.2807, 0.3011, 0.3361, 0.3024, 0.2285, 0.291, 0.1316, 0.1151, 0.3404, 0.5562, 0.6379, 0.6553, 0.7384, 0.6534, 0.5423, 0.6877, 0.7325, 0.7726, 0.8229, 0.8787, 0.9108, 0.6705, 0.6092, 0.7505, 0.4775, 0.1666, 0.3749, 0.3776, 0.2106, 0.5886, 0.5628, 0.2577, 0.5245, 0.6149, 0.5123, 0.3385, 0.1499, 0.0546, 0.027, 0.038, 0.0339, 0.0149, 0.0335, 0.0376, 0.0174, 0.0132, 0.0103, 0.0364, 0.0208, 1.0], [0.0336, 0.0294, 0.0476, 0.0539, 0.0794, 0.0804, 0.1136, 0.1228, 0.1235, 0.0842, 0.0357, 0.0689, 0.1705, 0.3257, 0.4602, 0.6225, 0.7327, 0.7843, 0.7988, 0.8261, 1.0, 0.9814, 0.962, 0.9601, 0.9118, 0.9086, 0.7931, 0.5877, 0.3474, 0.4235, 0.4633, 0.341, 0.2849, 0.2847, 0.1742, 0.0549, 0.1192, 0.1154, 0.0855, 0.1811, 0.1264, 0.0799, 0.0378, 0.1268, 0.1125, 0.0505, 0.0949, 0.0677, 0.0259, 0.017, 0.0033, 0.015, 0.0111, 0.0032, 0.0035, 0.0169, 0.0137, 0.0015, 0.0069, 0.0051, 0.0], [0.0423, 0.0321, 0.0709, 0.0108, 0.107, 0.0973, 0.0961, 0.1323, 0.2462, 0.2696, 0.3412, 0.4292, 0.3682, 0.394, 0.2965, 0.3172, 0.2825, 0.305, 0.2408, 0.542, 0.6802, 0.632, 0.5824, 0.6805, 0.5984, 0.8412, 0.9911, 0.9187, 0.8005, 0.6713, 0.5632, 0.7332, 0.6038, 0.2575, 0.0349, 0.1799, 0.3039, 0.476, 0.5756, 0.4254, 0.5046, 0.7179, 0.6163, 0.5663, 0.5749, 0.3593, 0.2526, 0.2299, 0.1271, 0.0356, 0.0367, 0.0176, 0.0035, 0.0093, 0.0121, 0.0075, 0.0056, 0.0021, 0.0043, 0.0017, 1.0], [0.0208, 0.0186, 0.0131, 0.0211, 0.061, 0.0613, 0.0612, 0.0506, 0.0989, 0.1093, 0.1063, 0.1179, 0.1291, 0.1591, 0.168, 0.1918, 0.1615, 0.1647, 0.1397, 0.1426, 0.2429, 0.2816, 0.429, 0.6443, 0.9061, 1.0, 0.8087, 0.6119, 0.526, 0.3677, 0.2746, 0.102, 0.1339, 0.1582, 0.1952, 0.1787, 0.0429, 0.1096, 0.1762, 0.2481, 0.315, 0.292, 0.1902, 0.0696, 0.0758, 0.091, 0.0441, 0.0244, 0.0265, 0.0095, 0.014, 0.0074, 0.0063, 0.0081, 0.0087, 0.0044, 0.0028, 0.0019, 0.0049, 0.0023, 0.0], [0.0131, 0.0068, 0.0308, 0.0311, 0.0085, 0.0767, 0.0771, 0.064, 0.0726, 0.0901, 0.075, 0.0844, 0.1226, 0.1619, 0.2317, 0.2934, 0.3526, 0.3657, 0.3221, 0.3093, 0.4084, 0.4285, 0.4663, 0.5956, 0.6948, 0.8386, 0.8875, 0.6404, 0.3308, 0.3425, 0.492, 0.4592, 0.3034, 0.4366, 0.5175, 0.5122, 0.4746, 0.4902, 0.4603, 0.446, 0.4196, 0.2873, 0.2296, 0.0949, 0.0095, 0.0527, 0.0383, 0.0107, 0.0108, 0.0077, 0.0109, 0.0062, 0.0028, 0.004, 0.0075, 0.0039, 0.0053, 0.0013, 0.0052, 0.0023, 0.0], [0.053, 0.0885, 0.1997, 0.2604, 0.3225, 0.2247, 0.0617, 0.2287, 0.095, 0.074, 0.161, 0.2226, 0.2703, 0.3365, 0.4266, 0.4144, 0.5655, 0.6921, 0.8547, 0.9234, 0.9171, 1.0, 0.9532, 0.9101, 0.8337, 0.7053, 0.6534, 0.4483, 0.246, 0.202, 0.1446, 0.0994, 0.151, 0.2392, 0.4434, 0.5023, 0.4441, 0.4571, 0.3927, 0.29, 0.3408, 0.499, 0.3632, 0.1387, 0.18, 0.1299, 0.0523, 0.0817, 0.0469, 0.0114, 0.0299, 0.0244, 0.0199, 0.0257, 0.0082, 0.0151, 0.0171, 0.0146, 0.0134, 0.0056, 1.0], [0.0516, 0.0944, 0.0622, 0.0415, 0.0995, 0.2431, 0.1777, 0.2018, 0.2611, 0.1294, 0.2646, 0.2778, 0.4432, 0.3672, 0.2035, 0.2764, 0.3252, 0.1536, 0.2784, 0.3508, 0.5187, 0.7052, 0.7143, 0.6814, 0.51, 0.5308, 0.6131, 0.8388, 0.9031, 0.8607, 0.9656, 0.9168, 0.7132, 0.6898, 0.731, 0.4134, 0.158, 0.1819, 0.1381, 0.296, 0.6935, 0.8246, 0.5351, 0.4403, 0.6448, 0.6214, 0.3016, 0.1379, 0.0364, 0.0355, 0.0456, 0.0432, 0.0274, 0.0152, 0.012, 0.0129, 0.002, 0.0109, 0.0074, 0.0078, 1.0], [0.0134, 0.0172, 0.0178, 0.0363, 0.0444, 0.0744, 0.08, 0.0456, 0.0368, 0.125, 0.2405, 0.2325, 0.2523, 0.1472, 0.0669, 0.11, 0.2353, 0.3282, 0.4416, 0.5167, 0.6508, 0.7793, 0.7978, 0.7786, 0.8587, 0.9321, 0.9454, 0.8645, 0.722, 0.485, 0.1357, 0.2951, 0.4715, 0.6036, 0.8083, 0.987, 0.88, 0.6411, 0.4276, 0.2702, 0.2642, 0.3342, 0.4335, 0.4542, 0.396, 0.2525, 0.1084, 0.0372, 0.0286, 0.0099, 0.0046, 0.0094, 0.0048, 0.0047, 0.0016, 0.0008, 0.0042, 0.0024, 0.0027, 0.0041, 1.0], [0.01, 0.0194, 0.0155, 0.0489, 0.0839, 0.1009, 0.1627, 0.2071, 0.2696, 0.299, 0.3242, 0.3565, 0.3951, 0.5201, 0.6953, 0.8468, 1.0, 0.9278, 0.851, 0.801, 0.8142, 0.8825, 0.7302, 0.6107, 0.7159, 0.8458, 0.6319, 0.4808, 0.6291, 0.7152, 0.6005, 0.4235, 0.4106, 0.3992, 0.173, 0.1975, 0.237, 0.1339, 0.1583, 0.3151, 0.1968, 0.2054, 0.1272, 0.1129, 0.1946, 0.2195, 0.193, 0.1498, 0.0773, 0.0196, 0.0122, 0.013, 0.0073, 0.0077, 0.0075, 0.006, 0.008, 0.0019, 0.0053, 0.0019, 0.0], [0.1083, 0.107, 0.0257, 0.0837, 0.0748, 0.1125, 0.3322, 0.459, 0.5526, 0.5966, 0.5304, 0.2251, 0.2402, 0.2689, 0.6646, 0.6632, 0.1674, 0.0837, 0.4331, 0.8718, 0.7992, 0.3712, 0.1703, 0.1611, 0.2086, 0.2847, 0.2211, 0.6134, 0.5807, 0.6925, 0.3825, 0.4303, 0.7791, 0.8703, 1.0, 0.9212, 0.9386, 0.9303, 0.7314, 0.4791, 0.2087, 0.2016, 0.1669, 0.2872, 0.4374, 0.3097, 0.1578, 0.0553, 0.0334, 0.0209, 0.0172, 0.018, 0.011, 0.0234, 0.0276, 0.0032, 0.0084, 0.0122, 0.0082, 0.0143, 1.0], [0.0233, 0.0394, 0.0416, 0.0547, 0.0993, 0.1515, 0.1674, 0.1513, 0.1723, 0.2078, 0.1239, 0.0236, 0.1771, 0.3115, 0.499, 0.6707, 0.7655, 0.8485, 0.9805, 1.0, 1.0, 0.9992, 0.9067, 0.6803, 0.5103, 0.4716, 0.498, 0.6196, 0.7171, 0.6316, 0.3554, 0.2897, 0.4316, 0.3791, 0.2421, 0.0944, 0.0351, 0.0844, 0.0436, 0.113, 0.2045, 0.1937, 0.0834, 0.1502, 0.1675, 0.1058, 0.1111, 0.0849, 0.0596, 0.0201, 0.0071, 0.0104, 0.0062, 0.0026, 0.0025, 0.0061, 0.0038, 0.0101, 0.0078, 0.0006, 1.0], [0.0131, 0.0387, 0.0329, 0.0078, 0.0721, 0.1341, 0.1626, 0.1902, 0.261, 0.3193, 0.3468, 0.3738, 0.3055, 0.1926, 0.1385, 0.2122, 0.2758, 0.4576, 0.6487, 0.7154, 0.801, 0.7924, 0.8793, 1.0, 0.9865, 0.9474, 0.9474, 0.9315, 0.8326, 0.6213, 0.3772, 0.2822, 0.2042, 0.219, 0.2223, 0.1327, 0.0521, 0.0618, 0.1416, 0.146, 0.0846, 0.1055, 0.1639, 0.1916, 0.2085, 0.2335, 0.1964, 0.13, 0.0633, 0.0183, 0.0137, 0.015, 0.0076, 0.0032, 0.0037, 0.0071, 0.004, 0.0009, 0.0015, 0.0085, 1.0], [0.0228, 0.0853, 0.1, 0.0428, 0.1117, 0.1651, 0.1597, 0.2116, 0.3295, 0.3517, 0.333, 0.3643, 0.402, 0.4731, 0.5196, 0.6573, 0.8426, 0.8476, 0.8344, 0.8453, 0.7999, 0.8537, 0.9642, 1.0, 0.9357, 0.9409, 0.907, 0.7104, 0.632, 0.5667, 0.3501, 0.2447, 0.1698, 0.329, 0.3674, 0.2331, 0.2413, 0.2556, 0.1892, 0.194, 0.3074, 0.2785, 0.0308, 0.1238, 0.1854, 0.1753, 0.1079, 0.0728, 0.0242, 0.0191, 0.0159, 0.0172, 0.0191, 0.026, 0.014, 0.0125, 0.0116, 0.0093, 0.0012, 0.0036, 1.0], [0.0442, 0.0477, 0.0049, 0.0581, 0.0278, 0.0678, 0.1664, 0.149, 0.0974, 0.1268, 0.1109, 0.2375, 0.2007, 0.214, 0.1109, 0.2036, 0.2468, 0.6682, 0.8345, 0.8252, 0.8017, 0.8982, 0.9664, 0.8515, 0.6626, 0.3241, 0.2054, 0.5669, 0.5726, 0.4877, 0.7532, 0.76, 0.5185, 0.412, 0.556, 0.5569, 0.1336, 0.3831, 0.4611, 0.433, 0.2556, 0.1466, 0.3489, 0.2659, 0.0944, 0.137, 0.1344, 0.0416, 0.0719, 0.0637, 0.021, 0.0204, 0.0216, 0.0135, 0.0055, 0.0073, 0.008, 0.0105, 0.0059, 0.0105, 0.0], [0.0856, 0.0454, 0.0382, 0.0203, 0.0385, 0.0534, 0.214, 0.311, 0.2837, 0.2751, 0.2707, 0.0946, 0.102, 0.4519, 0.6737, 0.6699, 0.7066, 0.5632, 0.3785, 0.2721, 0.5297, 0.7697, 0.8643, 0.9304, 0.9372, 0.6247, 0.6024, 0.681, 0.5047, 0.5775, 0.4754, 0.24, 0.2779, 0.1997, 0.5305, 0.7409, 0.7775, 0.4424, 0.1416, 0.3508, 0.4482, 0.4208, 0.3054, 0.2235, 0.2611, 0.2798, 0.2392, 0.2021, 0.1326, 0.0358, 0.0128, 0.0172, 0.0138, 0.0079, 0.0037, 0.0051, 0.0258, 0.0102, 0.0037, 0.0037, 0.0], [0.0107, 0.0453, 0.0289, 0.0713, 0.1075, 0.1019, 0.1606, 0.2119, 0.3061, 0.2936, 0.3104, 0.3431, 0.2456, 0.1887, 0.1184, 0.208, 0.2736, 0.3274, 0.2344, 0.126, 0.0576, 0.1241, 0.3239, 0.4357, 0.5734, 0.7825, 0.9252, 0.9349, 0.9348, 1.0, 0.9308, 0.8478, 0.7605, 0.704, 0.7539, 0.799, 0.7673, 0.5955, 0.4731, 0.484, 0.434, 0.3954, 0.4837, 0.5379, 0.4485, 0.2674, 0.1541, 0.1359, 0.0941, 0.0261, 0.0079, 0.0164, 0.012, 0.0113, 0.0021, 0.0097, 0.0072, 0.006, 0.0017, 0.0036, 1.0], [0.0408, 0.0653, 0.0397, 0.0604, 0.0496, 0.1817, 0.1178, 0.1024, 0.0583, 0.2176, 0.2459, 0.3332, 0.3087, 0.2613, 0.3232, 0.3731, 0.4203, 0.5364, 0.7062, 0.8196, 0.8835, 0.8299, 0.7609, 0.7605, 0.8367, 0.8905, 0.7652, 0.5897, 0.3037, 0.0823, 0.2787, 0.7241, 0.8032, 0.805, 0.7676, 0.7468, 0.6253, 0.173, 0.2916, 0.5003, 0.522, 0.4824, 0.4004, 0.3877, 0.1651, 0.0442, 0.0663, 0.0418, 0.0475, 0.0235, 0.0066, 0.0062, 0.0129, 0.0184, 0.0069, 0.0198, 0.0199, 0.0102, 0.007, 0.0055, 0.0], [0.0177, 0.03, 0.0288, 0.0394, 0.063, 0.0526, 0.0688, 0.0633, 0.0624, 0.0613, 0.168, 0.3476, 0.4561, 0.5188, 0.6308, 0.7201, 0.5153, 0.3818, 0.2644, 0.3345, 0.4865, 0.6628, 0.7389, 0.9213, 1.0, 0.775, 0.5593, 0.6172, 0.8635, 0.6592, 0.477, 0.4983, 0.333, 0.3076, 0.2876, 0.2226, 0.0794, 0.0603, 0.1049, 0.0606, 0.153, 0.0983, 0.1643, 0.1901, 0.1107, 0.1917, 0.1467, 0.0392, 0.0356, 0.027, 0.0168, 0.0102, 0.0122, 0.0044, 0.0075, 0.0124, 0.0099, 0.0057, 0.0032, 0.0019, 0.0], [0.0115, 0.015, 0.0136, 0.0076, 0.0211, 0.1058, 0.1023, 0.044, 0.0931, 0.0734, 0.074, 0.0622, 0.1055, 0.1183, 0.1721, 0.2584, 0.3232, 0.3817, 0.4243, 0.4217, 0.4449, 0.4075, 0.3306, 0.4012, 0.4466, 0.5218, 0.7552, 0.9503, 1.0, 0.9084, 0.8283, 0.7571, 0.7262, 0.6152, 0.568, 0.5757, 0.5324, 0.3672, 0.1669, 0.0866, 0.0646, 0.1891, 0.2683, 0.2887, 0.2341, 0.1668, 0.1015, 0.1195, 0.0704, 0.0167, 0.0107, 0.0091, 0.0016, 0.0084, 0.0064, 0.0026, 0.0029, 0.0037, 0.007, 0.0041, 0.0], [0.0346, 0.0509, 0.0079, 0.0243, 0.0432, 0.0735, 0.0938, 0.1134, 0.1228, 0.1508, 0.1809, 0.239, 0.2947, 0.2866, 0.401, 0.5325, 0.5486, 0.5823, 0.6041, 0.6749, 0.7084, 0.789, 0.9284, 0.9781, 0.9738, 1.0, 0.9702, 0.9956, 0.8235, 0.602, 0.5342, 0.4867, 0.3526, 0.1566, 0.0946, 0.1613, 0.2824, 0.339, 0.3019, 0.2945, 0.2978, 0.2676, 0.2055, 0.2069, 0.1625, 0.1216, 0.1013, 0.0744, 0.0386, 0.005, 0.0146, 0.004, 0.0122, 0.0107, 0.0112, 0.0102, 0.0052, 0.0024, 0.0079, 0.0031, 1.0], [0.0329, 0.0216, 0.0386, 0.0627, 0.1158, 0.1482, 0.2054, 0.1605, 0.2532, 0.2672, 0.3056, 0.3161, 0.2314, 0.2067, 0.1804, 0.2808, 0.4423, 0.5947, 0.6601, 0.5844, 0.4539, 0.4789, 0.5646, 0.5281, 0.7115, 1.0, 0.9564, 0.609, 0.5112, 0.4, 0.0482, 0.1852, 0.2186, 0.1436, 0.1757, 0.1428, 0.1644, 0.3089, 0.3648, 0.4441, 0.3859, 0.2813, 0.1238, 0.0953, 0.1201, 0.0825, 0.0618, 0.0141, 0.0108, 0.0124, 0.0104, 0.0095, 0.0151, 0.0059, 0.0015, 0.0053, 0.0016, 0.0042, 0.0053, 0.0074, 1.0], [0.0093, 0.0269, 0.0217, 0.0339, 0.0305, 0.1172, 0.145, 0.0638, 0.074, 0.136, 0.2132, 0.3738, 0.3738, 0.2673, 0.2333, 0.5367, 0.7312, 0.7659, 0.6271, 0.4395, 0.433, 0.4326, 0.5544, 0.736, 0.8589, 0.8989, 0.942, 0.9401, 0.9379, 0.8575, 0.7284, 0.67, 0.7547, 0.8773, 0.9919, 0.9922, 0.9419, 0.8388, 0.6605, 0.4816, 0.2917, 0.1769, 0.1136, 0.0701, 0.1578, 0.1938, 0.1106, 0.0693, 0.0176, 0.0205, 0.0309, 0.0212, 0.0091, 0.0056, 0.0086, 0.0092, 0.007, 0.0116, 0.006, 0.011, 0.0], [0.0164, 0.0627, 0.0738, 0.0608, 0.0233, 0.1048, 0.1338, 0.0644, 0.1522, 0.078, 0.1791, 0.2681, 0.1788, 0.1039, 0.198, 0.3234, 0.3748, 0.2586, 0.368, 0.3508, 0.5606, 0.5231, 0.5469, 0.6954, 0.6352, 0.6757, 0.8499, 0.8025, 0.6563, 0.8591, 0.6655, 0.5369, 0.3118, 0.3763, 0.2801, 0.0875, 0.3319, 0.4237, 0.1801, 0.3743, 0.4627, 0.1614, 0.2494, 0.3202, 0.2265, 0.1146, 0.0476, 0.0943, 0.0824, 0.0171, 0.0244, 0.0258, 0.0143, 0.0226, 0.0187, 0.0185, 0.011, 0.0094, 0.0078, 0.0112, 1.0], [0.0231, 0.0315, 0.017, 0.0226, 0.041, 0.0116, 0.0223, 0.0805, 0.2365, 0.2461, 0.2245, 0.152, 0.1732, 0.3099, 0.438, 0.5595, 0.682, 0.6164, 0.6803, 0.8435, 0.9921, 1.0, 0.7983, 0.5426, 0.3952, 0.5179, 0.565, 0.3042, 0.1881, 0.396, 0.2286, 0.3544, 0.4187, 0.2398, 0.1847, 0.376, 0.4331, 0.3626, 0.2519, 0.187, 0.1046, 0.2339, 0.1991, 0.11, 0.0684, 0.0303, 0.0674, 0.0785, 0.0455, 0.0246, 0.0151, 0.0125, 0.0036, 0.0123, 0.0043, 0.0114, 0.0052, 0.0091, 0.0008, 0.0092, 1.0], [0.0067, 0.0096, 0.0024, 0.0058, 0.0197, 0.0618, 0.0432, 0.0951, 0.0836, 0.118, 0.0978, 0.0909, 0.0656, 0.0593, 0.0832, 0.1297, 0.2038, 0.3811, 0.4451, 0.5224, 0.5911, 0.6566, 0.6308, 0.5998, 0.4958, 0.5647, 0.6906, 0.8513, 1.0, 0.9166, 0.7676, 0.6177, 0.5468, 0.5516, 0.5463, 0.5515, 0.4561, 0.3466, 0.3384, 0.2853, 0.2502, 0.1641, 0.1605, 0.1491, 0.1326, 0.0687, 0.0602, 0.0561, 0.0306, 0.0154, 0.0029, 0.0048, 0.0023, 0.002, 0.004, 0.0019, 0.0034, 0.0034, 0.0051, 0.0031, 0.0]]

for n_trees in [1, 5, 10, 50]:
    scores = evaluate_algorithm(dataset, bagging, n_folds, max_depth, min_size, sample_size, n_trees)
    print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))